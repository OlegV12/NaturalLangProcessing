{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW_9.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPUJdeOelh0wOzBoU7reYbN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OlegV12/NaturalLangProcessing/blob/Lesson_9/HW_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3whEfhqm1G1",
        "outputId": "602e47b7-b021-43d8-db3a-7fc7f5fad2e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from torch.autograd import Variable"
      ],
      "metadata": {
        "id": "0XrdJDKInBWx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1M2kE5x4v_xq",
        "outputId": "d4b622ea-16ac-43dc-aa32-3c023c824cb9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = open('/gdrive/MyDrive/Colab Notebooks/NLP/evgenyi_onegin.txt', 'rb').read().decode(encoding='utf-8')"
      ],
      "metadata": {
        "id": "OrzE8UN1tFNH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(text))"
      ],
      "metadata": {
        "id": "eRtvEKjBwKtt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text.split()"
      ],
      "metadata": {
        "id": "2luo3FStwgN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence = text.split()"
      ],
      "metadata": {
        "id": "WoxJdcI4xG92"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n",
        "            for i in range(len(test_sentence) - 2)]\n",
        "chunk_len=len(trigrams)\n",
        "print(trigrams[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vXSXl3vxPvb",
        "outputId": "769d1542-68c3-49ae-cd1a-292786a862b6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(['Александр', 'Сергеевич'], 'Пушкин'), (['Сергеевич', 'Пушкин'], 'Евгений'), (['Пушкин', 'Евгений'], 'Онегин')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = set(test_sentence)\n",
        "voc_len=len(vocab)\n",
        "word_to_ix = {word: i for i, word in enumerate(vocab)}"
      ],
      "metadata": {
        "id": "fYtwtb0qxWTb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "voc_len"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3f9l2I133TV",
        "outputId": "0d48eab2-058b-4dee-cd3c-1df1461b6ac9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11512"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inp=[]\n",
        "tar=[]\n",
        "for context, target in trigrams:\n",
        "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
        "        inp.append(context_idxs)\n",
        "        targ = torch.tensor([word_to_ix[target]], dtype=torch.long)\n",
        "        tar.append(targ)"
      ],
      "metadata": {
        "id": "HT8O1E4NxanQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time, math\n",
        "\n",
        "def time_since(since):\n",
        "    s = time.time() - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)"
      ],
      "metadata": {
        "id": "fV6NnM-p4oWd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "        super(RNN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size*2, hidden_size, n_layers,batch_first=True,\n",
        "                          bidirectional=False)\n",
        "        self.decoder = nn.Linear(hidden_size, output_size)\n",
        "    \n",
        "    def forward(self, input, hidden):\n",
        "        input = self.encoder(input.view(1, -1))\n",
        "        output, hidden = self.gru(input.view(1, 1, -1), hidden)\n",
        "        output = self.decoder(output.view(1, -1))\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))"
      ],
      "metadata": {
        "id": "aUw_Wp9i-QM6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(inp, target):\n",
        "    hidden = decoder.init_hidden().cuda()\n",
        "    decoder.zero_grad()\n",
        "    loss = 0\n",
        "    \n",
        "    for c in range(chunk_len):\n",
        "        output, hidden = decoder(inp[c].cuda(), hidden)\n",
        "        loss += criterion(output, target[c].cuda())\n",
        "\n",
        "    loss.backward()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.data.item() / chunk_len"
      ],
      "metadata": {
        "id": "rtIfR9Zy-2Lz"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 50\n",
        "print_every = 10\n",
        "plot_every = 10\n",
        "hidden_size = 100\n",
        "n_layers = 1\n",
        "lr = 0.015\n",
        "\n",
        "decoder = RNN(voc_len, hidden_size, voc_len, n_layers)\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "start = time.time()\n",
        "all_losses = []\n",
        "loss_avg = 0\n",
        "decoder.cuda()\n",
        "for epoch in range(n_epochs):\n",
        "    loss = train(inp, tar)       \n",
        "    loss_avg += loss\n",
        "\n",
        "    if epoch % print_every == 0:\n",
        "        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 50, loss))\n",
        "\n",
        "\n",
        "    if epoch % plot_every == 0:\n",
        "        all_losses.append(loss_avg / plot_every)\n",
        "        loss_avg = 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mREPoQA-oy7",
        "outputId": "ab323352-8d85-4db7-84ec-151c551ddc63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0m 22s (0 0%) 9.3843]\n",
            "[3m 8s (10 10%) 4.2287]\n",
            "[5m 54s (20 20%) 1.0310]\n",
            "[8m 40s (30 30%) 0.2352]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(prime_str, predict_len=100, temperature=0.8):\n",
        "    hidden = decoder.init_hidden().cuda()\n",
        "\n",
        "    for p in range(predict_len):\n",
        "        \n",
        "        prime_input = torch.tensor([word_to_ix[w] for w in prime_str.split()], dtype=torch.long).cuda()\n",
        "        inp = prime_input[-2:] #last two words as input\n",
        "        output, hidden = decoder(inp, hidden)\n",
        "\n",
        "        output_dist = output.data.view(-1).div(temperature).exp()\n",
        "        top_i = torch.multinomial(output_dist, 1)[0]\n",
        "        \n",
        "        # Add predicted word to string and use as next input\n",
        "        predicted_word = list(word_to_ix.keys())[list(word_to_ix.values()).index(top_i)]\n",
        "        prime_str += \" \" + predicted_word\n",
        "\n",
        "    return prime_str"
      ],
      "metadata": {
        "id": "zdC_J010KLsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(evaluate('он думал', 40, temperature=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9KT_4DPKPy6",
        "outputId": "6b97c060-005a-421d-a0a0-cb3441b3738f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "он думал да молвил от утренних долин В ученым прямо и теплотой Камин чуть дышит. Дым из трубок В трубу уходит. Светлый кубок Еще шипит среди стола. Вечерняя находит мгла... (Люблю я дружеские враки И дружеский бокал вина Порою той, что названа\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(evaluate('он думал', 40, temperature=0.5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjZMgQMBND0A",
        "outputId": "1e58ea7d-5971-42d0-9088-a7f54be2a794"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "он думал бродит, Меня смотрит был мой Онегин поскакал. Перед померкшими домами Вдоль сонной улицы рядами Двойные фонари карет Веселый изливают свет И радуги на снег наводят; Усеян плошками кругом, Блестит великолепный дом; По цельным окнам тени ходят, Мелькают профили голов И\n"
          ]
        }
      ]
    }
  ]
}